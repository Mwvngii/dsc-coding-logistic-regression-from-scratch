








# Your code here
import numpy as np

def predict_y(X, w):
    return np.dot(X, w)





# Your code here
def sigmoid(x):
    return 1/(1+np.exp(-x))





import matplotlib.pyplot as plt
%matplotlib inline
x = np.linspace(-20, 20, 10000)
y = sigmoid(x)

# Plot sigmoid
plt.plot(x,y)
plt.title('Sigmoid Function')
plt.xlabel('x')
plt.ylabel('sigmoid(x)')
plt.show()





# Your code here
def grad_desc(X, y, max_iterations, alpha, initial_weights=None):
    if initial_weights is None:
        initial_weights = np.ones(X.shape[1])
    """Be sure to set default behavior for the initial_weights parameter."""
    # Create a for loop of iterations
    weights = initial_weights
    m = len(y)

    for i in range(max_iterations):
        # Generate predictions using the current feature weights
        predictions = sigmoid(np.dot(X, weights))
        
        # Calculate an error vector based on these initial predictions and the correct labels
        error_vector = predictions - y
        
        # Calculate the gradient
        # As we saw in the previous lab, calculating the gradient is often the most difficult task.
        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE
        # For more details on the derivation, see the additional resources section below.
        gradient = np.dot(X.transpose(), error_vector)/m
        
        # Update the weight vector take a step of alpha in direction of gradient 
        weights = alpha * gradient
    
    # Return finalized weights
    return weights
    





# Import data
import pandas as pd
df = pd.read_csv('heart.csv')

# Create the predictor and target variables
y = df['target']
X = df.drop(columns=['target'], axis=1)

print(y.value_counts())
X.head()





# Your code here
alpha = 0.01
max_iterations = 10000

final_weights = grad_desc(X.values, y.values, max_iterations, alpha)

print("Final weights from our gradient descent logistic regression:", final_weights)





# Your code here
from sklearn.linear_model import LogisticRegression

# Initializing Log Regression
log_reg = LogisticRegression(fit_intercept=False, C=1e16, random_state=2, solver='liblinear')

# Fit model
log_reg.fit(X,y)






# Your code here
# Compare the coefficient weights of your model to that generated by scikit-learn.
print("Weights from scikit-learn logistic regression:", log_reg.coef_)

print("Final weights from gradient descent:", final_weights)





# Your code here
def grad_desc_with_cost(X, y, max_iterations, alpha, initial_weights=None):
    if initial_weights is None:
        initial_weights = np.ones(X.shape[1])
    
    weights = initial_weights
    m = len(y)
    costs = []
    
    for i in range(max_iterations):
        # Make predictions
        predictions = sigmoid(np.dot(X, weights))
        
        # Calculate the error
        error_vector = predictions - y
        
        # Compute the gradient
        gradient = np.dot(X.T, error_vector) / m
        
        # Update weights
        weights -= alpha * gradient
        
        # Calculate the cost (log-loss)
        cost = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))
        costs.append(cost)
    
    return weights, costs

# Run the updated gradient descent
final_weights, costs = grad_desc_with_cost(X.values, y.values, max_iterations, alpha)

# Plot the cost over iterations
plt.plot(range(max_iterations), costs)
plt.title('Cost vs Iterations')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.show()






